{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMvMSYOS0SQxxbYng6DZTUC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["semi-supervised domain adaptation, the minimax entropy approach alternates between:"],"metadata":{"id":"rqfqWgZIowup"}},{"cell_type":"markdown","source":["**Maximizing entropy** forces the classifier to remain uncertain when predicting the unlabeled target samples. This discourages it from making confident but incorrect predictions, preventing bias toward the source domain.\n","\n","**Minimizing entropy** ensures the feature extractor learns domain-invariant features, aligning the distributions of the source and target domains and making the classifier confident where it should be.\n"],"metadata":{"id":"gC4Cz7Aawkgo"}},{"cell_type":"markdown","source":["minimax entropy-based domain adaptation is feature-based. It focuses on aligning feature distributions between the source and target domains to reduce domain mismatch. By alternately maximizing and minimizing entropy, the method ensures that the features are both diverse and structured, enabling better generalization across domains"],"metadata":{"id":"srubmFGpqdI_"}},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"u6ymUsmIvOvj"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n","import torch.nn.functional as F\n","\n","from torchvision import datasets, transforms\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"lKWovNwv63ah","executionInfo":{"status":"ok","timestamp":1744932498774,"user_tz":300,"elapsed":18,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["**Data Processing**"],"metadata":{"id":"WGAwHxBbvSGe"}},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"mei1963/domainnet\")\n","\n","print(\"Path to dataset files:\", path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDNM6kdlCFbO","executionInfo":{"status":"ok","timestamp":1744932499987,"user_tz":300,"elapsed":1209,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"3d67da13-8cc9-44a8-d852-da9f45697d24"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /kaggle/input/domainnet\n"]}]},{"cell_type":"code","source":["def walk_through_dir(dir_path):\n","  \"\"\"\n","  Walks through dir_path returning its contents.\n","  Args:\n","    dir_path (str or pathlib.Path): target directory\n","\n","  Returns:\n","    A print out of:\n","      number of subdiretories in dir_path\n","      number of images (files) in each subdirectory\n","      name of each subdirectory\n","  \"\"\"\n","  for dirpath, dirnames, filenames in os.walk(dir_path):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"],"metadata":{"id":"bzodO1yvDWJf","executionInfo":{"status":"ok","timestamp":1744932499995,"user_tz":300,"elapsed":6,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# walk_through_dir(path)"],"metadata":{"id":"h25lXEIX3RZj","executionInfo":{"status":"ok","timestamp":1744932499997,"user_tz":300,"elapsed":1,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["source_path = os.path.join(path, \"DomainNet/real\")\n","target_path = os.path.join(path, \"DomainNet/sketch\")"],"metadata":{"id":"3ULT-irm-VQv","executionInfo":{"status":"ok","timestamp":1744932500000,"user_tz":300,"elapsed":1,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["simple_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","augmented_transform = transforms.Compose([\n","    # Random augmentations\n","    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally with a 50% chance\n","    transforms.RandomRotation(degrees=15),   # Randomly rotate images by up to Â±15 degrees\n","    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),  # Crop to 224x224 with random scale\n","\n","    # Color jitter to introduce brightness/contrast variation\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","\n","    # Convert to tensor and normalize\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n"],"metadata":{"id":"addTCBE6--Cf","executionInfo":{"status":"ok","timestamp":1744932500004,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["source_dataset = datasets.ImageFolder(root=source_path, transform=augmented_transform)\n","target_dataset = datasets.ImageFolder(root=target_path, transform=simple_transform)\n"],"metadata":{"id":"ytC71Bz0-0b-","executionInfo":{"status":"ok","timestamp":1744932501667,"user_tz":300,"elapsed":1662,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["#Making label and unlabed datasets for semi-supervised\n","\n","# Calculate sizes for the split\n","unlabeled_size = int(0.9 * len(target_dataset))\n","labeled_size = len(target_dataset) - unlabeled_size\n","\n","# Split the dataset\n","target_dataset_unlabeled, target_dataset_labeled = random_split(target_dataset, [unlabeled_size, labeled_size])"],"metadata":{"id":"JYxmGbJTI0wR","executionInfo":{"status":"ok","timestamp":1744932501672,"user_tz":300,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["class AugmentedDataset(Dataset):\n","    def __init__(self, dataset, transform):\n","        self.dataset = dataset\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, label = self.dataset[idx]  # Get original image and label\n","        image = self.transform(image)  # Apply augmentation\n","        return image, label"],"metadata":{"id":"qgWXdDJWRZcu","executionInfo":{"status":"ok","timestamp":1744932501695,"user_tz":300,"elapsed":19,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["target_dataset_labeled_augmented = AugmentedDataset(target_dataset_labeled, augmented_transform)\n"],"metadata":{"id":"RrMdj1LTRfAi","executionInfo":{"status":"ok","timestamp":1744932501748,"user_tz":300,"elapsed":50,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["len(target_dataset_labeled), len(target_dataset_unlabeled)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w34zN0f7LMuf","executionInfo":{"status":"ok","timestamp":1744932501785,"user_tz":300,"elapsed":30,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"9974e888-0795-4ad2-ec9d-5f78c918091d"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7039, 63347)"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["dataloader_source = DataLoader(source_dataset, batch_size=32, shuffle=True)\n","dataloader_target_labeled = DataLoader(target_dataset_labeled, batch_size=32, shuffle=True)\n","dataloader_target_unlabeled = DataLoader(target_dataset_unlabeled, batch_size=32, shuffle=True)"],"metadata":{"id":"X5JS02KhIlog","executionInfo":{"status":"ok","timestamp":1744932501789,"user_tz":300,"elapsed":2,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["class_names = source_dataset.classes\n","# print(class_names)\n","len(class_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EfS7dGPwB1Gs","executionInfo":{"status":"ok","timestamp":1744932501815,"user_tz":300,"elapsed":21,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"d54482a0-2e6d-4e25-fe09-23ee9c727c7b"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["345"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# img, label = next(iter(dataloader_target))\n","\n","# # Batch size will now be 1, try changing the batch_size parameter above and see what happens\n","# print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n","# print(f\"Label shape: {label.shape}\")"],"metadata":{"id":"vDkByZZ1_aeu","executionInfo":{"status":"ok","timestamp":1744932501819,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["Functions"],"metadata":{"id":"zQluaueq9MMm"}},{"cell_type":"code","source":["def entropy_maximization(predictions):\n","    \"\"\"\n","    Maximizes entropy for the given predictions.\n","    Args:\n","        predictions (torch.Tensor): Class probabilities (softmax outputs).\n","    Returns:\n","        torch.Tensor: Entropy loss.\n","    \"\"\"\n","    # Compute entropy for each sample (with clamping for numerical stability)\n","    entropy = -torch.sum(predictions * torch.log(torch.clamp(predictions, min=1e-6, max=1.0)), dim=1)\n","\n","    # Maximize entropy (minimize negative entropy)\n","    entropy_loss = torch.mean(entropy)\n","    return entropy_loss\n","\n","\n","def entropy_minimization(predictions):\n","    \"\"\"\n","    Minimizes entropy for the given predictions.\n","    Args:\n","        predictions (torch.Tensor): Class probabilities (softmax outputs).\n","    Returns:\n","        torch.Tensor: Entropy loss.\n","    \"\"\"\n","    # Compute entropy for each sample (with clamping for numerical stability)\n","    entropy = -torch.sum(predictions * torch.log(torch.clamp(predictions, min=1e-6, max=1.0)), dim=1)\n","\n","    # Minimize entropy (maximize negative entropy)\n","    entropy_loss = -torch.mean(entropy)\n","    return entropy_loss\n","\n","\n","def generate_pseudo_labels(predictions, confidence_threshold=0.9):\n","    \"\"\"\n","    Generate pseudo-labels for target domain data based on model predictions.\n","    Args:\n","        predictions (torch.Tensor): Class probabilities (softmax outputs).\n","        confidence_threshold (float): Minimum confidence to assign a pseudo-label.\n","    Returns:\n","        torch.Tensor: Pseudo-labels for confident predictions.\n","    \"\"\"\n","    # Get the predicted class and confidence for each sample\n","    confidences, pseudo_labels = torch.max(predictions, dim=1)\n","\n","    # Filter pseudo-labels based on confidence threshold\n","    mask = confidences >= confidence_threshold\n","    invalid_label = torch.tensor(-1, device=predictions.device)  # Define once\n","    pseudo_labels = torch.where(mask, pseudo_labels, invalid_label)  # Replace uncertain samples with -1\n","\n","    return pseudo_labels\n"],"metadata":{"id":"f6S6OKUbvTnK","executionInfo":{"status":"ok","timestamp":1744932501823,"user_tz":300,"elapsed":2,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["**Feature extractor**"],"metadata":{"id":"GJeTcIyl7cjl"}},{"cell_type":"code","source":["weights = ResNet50_Weights.DEFAULT\n","model = resnet50(weights=weights)\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","\n","model.fc = nn.Identity()"],"metadata":{"id":"ZjgkAbWi766M","executionInfo":{"status":"ok","timestamp":1744932502168,"user_tz":300,"elapsed":344,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["**Classifier**"],"metadata":{"id":"WuZQ-4LxChZu"}},{"cell_type":"code","source":["class Classifier(nn.Module):\n","    def __init__(self, input_dim, num_classes, hidden_dim=256):\n","        super(Classifier, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Fully connected layer\n","        self.relu = nn.ReLU()                       # Activation function\n","        self.dropout = nn.Dropout(p=0.5)            # Dropout for regularization\n","        self.fc2 = nn.Linear(hidden_dim, num_classes)  # Output layer for class probabilities\n","        # No Softmax here; raw logits will be returned\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        # x = self.dropout(x)  # Apply dropout\n","        x = self.fc2(x)       # Output logits\n","        return x              # Raw logits, suitable for CrossEntropyLoss\n"],"metadata":{"id":"lZsg3w-8Ckkr","executionInfo":{"status":"ok","timestamp":1744932502172,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["**Training Loop**"],"metadata":{"id":"QesrLAtABmxR"}},{"cell_type":"code","source":["feature_encoder = model.to('cuda')\n","classifier = Classifier(input_dim=2048, num_classes=345).to('cuda')\n","optimizer_encoder = optim.Adam(feature_encoder.parameters(), lr=0.0005)\n","optimizer_classifier = optim.Adam(classifier.parameters(), lr=0.0005)"],"metadata":{"id":"Ke3FYqgvBmCp","executionInfo":{"status":"ok","timestamp":1744932502198,"user_tz":300,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","\n","torch.autograd.set_detect_anomaly(True)\n","\n","\n","for epoch in range(num_epochs):\n","    feature_encoder.train()\n","    classifier.train()\n","\n","    total_supervised_loss_source = 0\n","    total_supervised_loss_target = 0\n","    total_entropy_loss_encoder = 0\n","    total_entropy_loss_classifier = 0\n","\n","    batch_count = 0\n","\n","    for (source_data, labeled_target_data, unlabeled_target_data) in zip(dataloader_source, dataloader_target_labeled, dataloader_target_unlabeled):\n","        batch_count += 1\n","\n","        # source domain labeled\n","        source_images, source_labels = source_data\n","        source_images, source_labels = source_images.to('cuda'), source_labels.to('cuda')\n","        features_source = feature_encoder(source_images)\n","        predictions_source = classifier(features_source)\n","        supervised_loss_source = F.cross_entropy(predictions_source, source_labels)\n","        total_supervised_loss_source += supervised_loss_source.item()\n","\n","\n","        # target domain labeled\n","        labeled_target_images, labels_target_labeled = labeled_target_data\n","        labeled_target_images, labels_target_labeled = labeled_target_images.to('cuda'), labels_target_labeled.to('cuda')\n","        features_target_labeled = feature_encoder(labeled_target_images)\n","        predictions_target_labeled = classifier(features_target_labeled)\n","        supervised_loss_target  = F.cross_entropy(predictions_target_labeled, labels_target_labeled)\n","        total_supervised_loss_target += supervised_loss_target.item()\n","\n","\n","        # target domain unlabeled\n","        unlabeled_target_images, _ = unlabeled_target_data\n","        unlabeled_target_images = unlabeled_target_images.to('cuda')\n","        features_target_unlabeled = feature_encoder(unlabeled_target_images)\n","        predictions_target_unlabeled = classifier(features_target_unlabeled)\n","        pseudo_labels = generate_pseudo_labels(predictions_target_unlabeled)\n","        entropy_loss_encoder = entropy_maximization(predictions_target_unlabeled)\n","        entropy_loss_classifier = entropy_minimization(predictions_target_unlabeled)\n","        total_entropy_loss_encoder += entropy_loss_encoder.item()\n","        total_entropy_loss_classifier += entropy_loss_classifier.item()\n","\n","\n","        # Combine all losses for a single backward pass\n","        scaled_entropy_loss_encoder = 0.01 * entropy_loss_encoder\n","        scaled_entropy_loss_classifier = 0.01 * entropy_loss_classifier\n","        total_loss = (\n","            supervised_loss_source\n","            + supervised_loss_target\n","            + scaled_entropy_loss_encoder\n","            + scaled_entropy_loss_classifier\n","        )\n","\n","\n","        # Optimize both encoder and classifier together\n","        optimizer_encoder.zero_grad()\n","        optimizer_classifier.zero_grad()\n","        total_loss.backward()\n","        optimizer_encoder.step()\n","        optimizer_classifier.step()\n","\n","    # Print epoch-level statistics\n","    print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n","    print(f\"  Source Supervised Loss: {total_supervised_loss_source / batch_count:.4f}\")\n","    print(f\"  Target Supervised Loss: {total_supervised_loss_target / batch_count:.4f}\")\n","    print(f\"  Encoder Entropy Loss: {total_entropy_loss_encoder / batch_count:.4f}\")\n","    print(f\"  Classifier Entropy Loss: {total_entropy_loss_classifier / batch_count:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gL8cpBZwGL0Z","executionInfo":{"status":"ok","timestamp":1744934651140,"user_tz":300,"elapsed":2148939,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"676f781b-5a65-48f4-8010-a4f0151ced8f"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10:\n","  Source Supervised Loss: 4.6682\n","  Target Supervised Loss: 5.0883\n","  Encoder Entropy Loss: -7782.7270\n","  Classifier Entropy Loss: 7782.7270\n","Epoch 2/10:\n","  Source Supervised Loss: 2.4713\n","  Target Supervised Loss: 3.6579\n","  Encoder Entropy Loss: -24775.7129\n","  Classifier Entropy Loss: 24775.7129\n","Epoch 3/10:\n","  Source Supervised Loss: 1.8579\n","  Target Supervised Loss: 3.0330\n","  Encoder Entropy Loss: -31384.7094\n","  Classifier Entropy Loss: 31384.7094\n","Epoch 4/10:\n","  Source Supervised Loss: 1.6433\n","  Target Supervised Loss: 2.6431\n","  Encoder Entropy Loss: -34079.8426\n","  Classifier Entropy Loss: 34079.8426\n","Epoch 5/10:\n","  Source Supervised Loss: 1.4783\n","  Target Supervised Loss: 2.3760\n","  Encoder Entropy Loss: -35529.7149\n","  Classifier Entropy Loss: 35529.7149\n","Epoch 6/10:\n","  Source Supervised Loss: 1.3915\n","  Target Supervised Loss: 2.1382\n","  Encoder Entropy Loss: -38047.6513\n","  Classifier Entropy Loss: 38047.6513\n","Epoch 7/10:\n","  Source Supervised Loss: 1.3931\n","  Target Supervised Loss: 1.9565\n","  Encoder Entropy Loss: -38546.6353\n","  Classifier Entropy Loss: 38546.6353\n","Epoch 8/10:\n","  Source Supervised Loss: 1.3619\n","  Target Supervised Loss: 1.7659\n","  Encoder Entropy Loss: -39489.3793\n","  Classifier Entropy Loss: 39489.3793\n","Epoch 9/10:\n","  Source Supervised Loss: 1.2996\n","  Target Supervised Loss: 1.6138\n","  Encoder Entropy Loss: -40575.3185\n","  Classifier Entropy Loss: 40575.3185\n","Epoch 10/10:\n","  Source Supervised Loss: 1.2693\n","  Target Supervised Loss: 1.4687\n","  Encoder Entropy Loss: -41772.8153\n","  Classifier Entropy Loss: 41772.8153\n"]}]},{"cell_type":"markdown","source":["**Conclusion**"],"metadata":{"id":"H6uDOCTSOVEG"}},{"cell_type":"markdown","source":["results confirm steady progress in supervised learning, with both source and target supervised losses decreasing consistently. This means the model is effectively leveraging labeled data from both domains. Meanwhile, your encoder and classifier entropy losses are growing, which aligns with the expected behavior in a Minimax Entropy framework. However, the rapid growth of entropy losses still warrants careful attention."],"metadata":{"id":"3GBLJf8GOTmn"}}]}